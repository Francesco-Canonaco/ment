{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2280ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import lingam\n",
    "from lingam.utils import print_causal_directions, print_dagc, make_dot\n",
    "from utilities import generate_connection_matrix, sample_uniform_noise, generate_dataset, sample_from_disjoint_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c162945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_adjacency_matrix(matrix, n=1, mode='both'):\n",
    "    \"\"\"\n",
    "    Perturbs a weighted adjacency matrix by adding/removing edges,\n",
    "    preserving causal order (i.e., j < i for edge j → i).\n",
    "\n",
    "    Parameters:\n",
    "    - matrix: np.ndarray, square weighted adjacency matrix\n",
    "    - n: number of edges to add/remove\n",
    "    - mode: 'add', 'remove', or 'both'\n",
    "    - weight_range: tuple, range of weights to assign when adding edges\n",
    "\n",
    "    Returns:\n",
    "    - new_matrix: np.ndarray, perturbed matrix\n",
    "    \"\"\"\n",
    "\n",
    "    new_matrix = matrix.copy()\n",
    "    num_nodes = new_matrix.shape[0]\n",
    "    \n",
    "    # change all non-zero weights (j < i) to have matrix with same causal order but different coefficients\n",
    "    '''for i in range(num_nodes):\n",
    "        for j in range(i):  # Ensure j < i\n",
    "            if new_matrix[i, j] != 0:\n",
    "                new_matrix[i, j] = sample_from_disjoint_interval(1)[0]'''\n",
    "\n",
    "    # Collect removable and addable edges\n",
    "    removable_edges = [(i, j) for i in range(num_nodes) for j in range(i) if new_matrix[i, j] != 0]\n",
    "    addable_edges = [(i, j) for i in range(num_nodes) for j in range(i) if new_matrix[i, j] == 0]\n",
    "    \n",
    "    if mode in ['remove', 'both']:\n",
    "        to_remove = random.sample(removable_edges, min(n, len(removable_edges)))\n",
    "        for i, j in to_remove:\n",
    "            new_matrix[i, j] = 0.0\n",
    "\n",
    "    if mode in ['add', 'both']:\n",
    "        to_add = random.sample(addable_edges, min(n, len(addable_edges)))\n",
    "        for i, j in to_add:\n",
    "            weight = sample_from_disjoint_interval(1)[0]\n",
    "            new_matrix[i, j] = weight\n",
    "\n",
    "    return new_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "af959df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "def perturb_adjacency_matrix(matrix, n=1, mode='both'):\n",
    "    \"\"\"\n",
    "    Perturbs a weighted adjacency matrix by changing existing weights and \n",
    "    optionally adding/removing edges, preserving causal order (j < i).\n",
    "\n",
    "    Parameters:\n",
    "    - matrix: np.ndarray, square weighted adjacency matrix\n",
    "    - n: int, number of edges to add/remove\n",
    "    - mode: str, one of 'add', 'remove', or 'both'\n",
    "\n",
    "    Returns:\n",
    "    - new_matrix: np.ndarray, perturbed matrix\n",
    "    \"\"\"\n",
    "    new_matrix = matrix.copy()\n",
    "    num_nodes = new_matrix.shape[0]\n",
    "    \n",
    "    # Get lower triangle indices (j < i)\n",
    "    i, j = np.tril_indices(num_nodes, k=-1)\n",
    "\n",
    "    # Reassign all existing weights (preserve structure but change values)\n",
    "    mask = new_matrix[i, j] != 0\n",
    "    new_weights = sample_from_disjoint_interval(mask.sum())\n",
    "    new_matrix[i[mask], j[mask]] = new_weights\n",
    "\n",
    "    # Get current removable and addable edges\n",
    "    removable_edges = list(zip(i[mask], j[mask]))\n",
    "    addable_edges = list(zip(i[~mask], j[~mask]))\n",
    "\n",
    "    # Remove edges\n",
    "    if mode in ['remove', 'both']:\n",
    "        to_remove = random.sample(removable_edges, min(n, len(removable_edges)))\n",
    "        rem_i, rem_j = zip(*to_remove) if to_remove else ([], [])\n",
    "        new_matrix[rem_i, rem_j] = 0.0\n",
    "\n",
    "    # Add edges\n",
    "    if mode in ['add', 'both']:\n",
    "        to_add = random.sample(addable_edges, min(n, len(addable_edges)))\n",
    "        add_i, add_j = zip(*to_add) if to_add else ([], [])\n",
    "        new_weights = sample_from_disjoint_interval(len(to_add)) if to_add else []\n",
    "        new_matrix[add_i, add_j] = new_weights\n",
    "\n",
    "    return new_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_causal_order(matrix):\n",
    "    G = nx.DiGraph()\n",
    "    num_nodes = matrix.shape[0]\n",
    "\n",
    "    # Add edges from matrix\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if matrix[i, j] != 0:\n",
    "                G.add_edge(j, i)  # edge from parent j to child i\n",
    "\n",
    "    # Compute topological sort\n",
    "    try:\n",
    "        causal_order = list(nx.topological_sort(G))\n",
    "        return causal_order\n",
    "    except nx.NetworkXUnfeasible:\n",
    "        raise ValueError(\"The graph contains cycles and is not a DAG.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61fe9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea3595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "21553974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_multigroup_data(p, s, c, n, PERT, shared_permutation=True, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    B = generate_connection_matrix(p, s)\n",
    "    perm = None\n",
    "    datasets = []\n",
    "    matrices = []\n",
    "    #sample_sizes = [rng.integers(int(10_000), int(15_000)) for _ in range(C)] #change 10_000 and 15_000 with 1.2*p and 2.5*p\n",
    "    sample_size = n\n",
    "    for g in range(c):\n",
    "        n_perturb = int(np.count_nonzero(B) * PERT)\n",
    "        Bc = perturb_adjacency_matrix(B, n_perturb, \"both\")\n",
    "        variances = rng.uniform(1, 3, size=p)\n",
    "        E = sample_uniform_noise(sample_size, p, variances)\n",
    "        X, _, perm = generate_dataset(E, Bc, sample_size, permutation=perm \n",
    "        if shared_permutation else None)\n",
    "        datasets.append(X)\n",
    "        matrices.append(Bc)\n",
    "    \n",
    "    return datasets, matrices, perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "62a840f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def average_squared_error(true_B, est_B):\n",
    "    # Flatten matrices and compute MSE\n",
    "    return mean_squared_error(true_B.flatten(), est_B.flatten())\n",
    "\n",
    "def invert_permutation(B, perm):\n",
    "    inverse_perm = np.argsort(perm)\n",
    "    B_perm_inverted = B[inverse_perm, :][:, inverse_perm]\n",
    "    return B_perm_inverted\n",
    "\n",
    "def compute_rescaling_matrix(adj_matrix: np.ndarray, X: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the rescaling matrix R such that:\n",
    "    B = R * B_tilde\n",
    "\n",
    "    Parameters:\n",
    "    - adj_matrix: np.ndarray, shape (n, n)\n",
    "        Adjacency matrix (non-zero entries represent edges i → j)\n",
    "    - X: pd.DataFrame, shape (n_samples, n_variables)\n",
    "        Original (non-normalized) data\n",
    "\n",
    "    Returns:\n",
    "    - R: np.ndarray, shape (n, n)\n",
    "        Rescaling matrix\n",
    "    \"\"\"\n",
    "    sums = X.sum().values\n",
    "    n = adj_matrix.shape[0]\n",
    "    R = np.zeros((n, n))\n",
    "    for i in range(n):         # row: target node\n",
    "        for j in range(n):     # col: source node\n",
    "            if adj_matrix[i, j] != 0:\n",
    "                R[i, j] = sums[i] / sums[j]\n",
    "    return R\n",
    "\n",
    "def normalize_by_column_sum(X):\n",
    "    \"\"\"\n",
    "    Normalize each column so that the sum of each column is 1.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: np.ndarray, shape (n_rows, n_columns)\n",
    "    \n",
    "    Returns:\n",
    "    - X_norm: np.ndarray, same shape as X\n",
    "    \"\"\"\n",
    "    col_sum = X.sum(axis=0)\n",
    "    col_sum[col_sum == 0] = 1  # Avoid division by zero\n",
    "    X_norm = X / col_sum\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "505234d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdt\n",
    "from cdt.metrics import SHD\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "21efc4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(p, s, n, r, PERT, c):\n",
    "    X, Bs_true, perms = simulate_multigroup_data(p, s, c, n, PERT)\n",
    "    X_normalized = [normalize_by_column_sum(X[0]), normalize_by_column_sum(X[1])]\n",
    "\n",
    "    model = lingam.MultiGroupDirectLiNGAM()\n",
    "    model.fit(X_normalized)\n",
    "\n",
    "    mse_cumulative = 0\n",
    "    shd_cumulative = 0\n",
    "\n",
    "    for g in range(c):\n",
    "        R = compute_rescaling_matrix(model.adjacency_matrices_[g], pd.DataFrame(X[g]))\n",
    "        B_rescaled = model.adjacency_matrices_[g] * R\n",
    "        B_est = invert_permutation(B_rescaled, perms)\n",
    "        mse_ = average_squared_error(Bs_true[g], B_est)\n",
    "        shd_ = SHD((Bs_true[g] != 0).astype(int), (B_est != 0).astype(int), True)\n",
    "        mse_cumulative += mse_\n",
    "        shd_cumulative += shd_\n",
    "\n",
    "    return {\n",
    "        'p': p,\n",
    "        's': s,\n",
    "        'n': n,\n",
    "        'c': c,\n",
    "        'perturbation': PERT,\n",
    "        'run': r,\n",
    "        'mse': mse_cumulative / c,\n",
    "        'shd': shd_cumulative / c\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = 10 # number of variables/features\n",
    "#s = 0.5 # edge probabilities (control the sparsity)\n",
    "#c = 2 #number of groups\n",
    "#n = 15_000 #number of samples\n",
    "#PERT = 0.15 #perturbation level (number of perturbation is |E|*PERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [50, 100, 150, 200] # number of variables/features\n",
    "S = [0.2, 0.25, 0.3] # edge probabilities (control the sparsity)\n",
    "N = [10_000, 20_000, 30_000] # number of samples \n",
    "PERT = 0.10 # perturbation level (number of perturbation is |E|*PERT)\n",
    "RUNS = 100 # number of runs for each triple: <P,S,N>\n",
    "c = 2 # number of groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "17e60dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n"
     ]
    }
   ],
   "source": [
    "# Run in parallel\n",
    "results_list = Parallel(n_jobs=-1)(  # set n_jobs=N to limit cores\n",
    "    delayed(run_experiment)(p, s, n, r, PERT, c)\n",
    "    for p, s, n in product(P, S, N)\n",
    "    for r in range(RUNS)\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "11c66ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS = 100\n",
    "results={'c':[], 'p':[], 's':[], 'perturbation':[], 'n':[], 'mse':[], 'shd':[], 'run':[]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924b20c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for p, s, n in product(P, S, N):\n",
    "    for r in range(RUNS):\n",
    "        print(f\"Running experiment with p={p}, s={s}, n={n}, run={r+1}/{RUNS}\")\n",
    "        X, Bs_true, perms = simulate_multigroup_data(p, s, c, n, PERT)\n",
    "        X_normalized = [normalize_by_column_sum(X[0]),normalize_by_column_sum(X[1])]\n",
    "        # fitting the model \n",
    "        model = lingam.MultiGroupDirectLiNGAM()\n",
    "        model.fit(X_normalized)\n",
    "        mse_cumulative = 0\n",
    "        shd_cumulative = 0\n",
    "        for g in range(0,c):\n",
    "            R = compute_rescaling_matrix(model.adjacency_matrices_[g], pd.DataFrame(X[g]))\n",
    "            B_rescaled = model.adjacency_matrices_[g] * R\n",
    "            B_est = invert_permutation(B_rescaled, perms)\n",
    "            mse_ = average_squared_error(Bs_true[g], B_est)\n",
    "            shd_ = SHD((Bs_true[g] != 0).astype(int), ( B_est != 0).astype(int), True) #double_for_anticausal (bool) – Count the badly oriented edges as two mistakes. Default: True\n",
    "            shd_cumulative += shd_\n",
    "            mse_cumulative += mse_\n",
    "\n",
    "        results['p'].append(p)\n",
    "        results['s'].append(s)\n",
    "        results['n'].append(n)\n",
    "        results['c'].append(c)\n",
    "        results['perturbation'].append(PERT)\n",
    "        results['mse'].append( mse_cumulative / c)\n",
    "        results['shd'].append(shd_cumulative / c)\n",
    "        results['run'].append(r)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c32d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ment_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
